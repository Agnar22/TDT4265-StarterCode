{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a) {-}\n",
    "\n",
    "### Logistic Regression {-}\n",
    "\n",
    "From the assignment we are given that:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = f(x) = \\frac{1}{1 + e^{-w^T x}}, \\quad w^T x = \\sum_{i=1}^{I} w_i x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    C(w) = \\frac{1}{N}\\sum_{n=1}^{N} C^n, \\quad C^n(w) = -\\left( y^n \\ln{(\\hat{y}^n)} + (1-\\hat{y}^n)\\ln{(1-\\hat{y}^n)} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "We want to find the gradient of the cost function for training example n, $C^n$, with respect to the weight, $w_i$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial C^n(w)}{\\partial w_i} &= \\frac{\\partial C^n(w)}{\\partial f(x^n)} \\frac{\\partial f(x^n)}{\\partial w_i} \\\\\n",
    "&= \\underbrace{-\\left(y^n \\frac{1}{\\hat{y}^n} + \\frac{1-y^n}{1-\\hat{y}^n}(-1)\\right)}_{\\frac{\\partial C^n(w)}{\\partial f(x^n)}} \\underbrace{x_i^n f(x^n) (1-f(x^n))}_{\\frac{\\partial f(x^n)}{\\partial w_i}}, \\quad f(x^n) = \\hat{y}^n \\\\\n",
    "&= -\\left( y^n(1-\\hat{y}^n) - (1-y^n)\\hat{y}^n \\right) x_i^n \\\\\n",
    "&= -(y^n-\\hat{y}^n) x_i^n \\quad \\blacksquare\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b) {-}\n",
    "\n",
    "### Softmax Regression {-}\n",
    "\n",
    "From the assignment we have that:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y}_k = \\frac{e^{z_k}}{\\sum_{k'=1}^{K} e^{z_{k'}}}, \\quad z_k = w_k^T x = \\sum_{i=1}^{I} w_{k,i} x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    C(w) = \\frac{1}{N}\\sum_{n=1}^{N} C^n, \\quad C^n(w) = - \\sum_{k=1}^{K} y_k^n \\ln{(\\hat{y}_k^n)}\n",
    "\\end{equation}\n",
    "\n",
    "We want to find the gradient of the cost function for training example n, $C^n$, with respect to the weight, $w_{kj}$:\n",
    "\n",
    "We start by rewriting the $C^n$ to another equivalent form\n",
    "\\begin{align*}\n",
    "    C^n(w) &= - \\sum_{k=1}^{K} y_k^n \\ln{\\left(\\frac{e^{z_k}}{\\sum_{k'=1}^{K} e^{z_{k'}}}\\right)} \\\\\n",
    "    &= -\\sum_{k=1}^{K} y_k^n \\ln{( e^{z_k})} + \\sum_{k=1}^{K} y_k^n \\ln{\\left( \\sum_{k'=1}^{K} e^{z_{k'}} \\right)} \\\\\n",
    "    &= -\\sum_{k=1}^{K} y_k^n \\underbrace{\\ln{( e^{z_k})}}_{z_k} + \\ln{\\left( \\sum_{k'=1}^{K} e^{z_{k'}} \\right)} \\underbrace{\\sum_{k=1}^{K} y_k^n}_{1}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\implies C^n(w) = -\\sum_{k=1}^{K} y_k^n z_k + \\ln{\\left( \\sum_{k'=1}^{K} e^{z_{k'}} \\right)}\n",
    "\\end{equation}\n",
    "\n",
    "Using the fact that that one $k'=k$ inside the logarithm, we get that:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C^n(w)}{\\partial w_{kj}} &= \\frac{\\partial C^n(w)}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}} \\\\\n",
    "    &= \\underbrace{\\left(-y_k^n + \\frac{e^{z_k}}{\\sum_{k'=1}^{K} e^{z_{k'}}}  \\right)}_{\\frac{\\partial C^n(w)}{\\partial z_k}} \\underbrace{x_j^n}_{\\frac{\\partial z_k}{\\partial w_{kj}}} \\\\\n",
    "    &= (-y_k^n + \\hat{y}_k^n) x_j^n\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C^n(w)}{\\partial w_{kj}} = -x_j^n (y_k^n - \\hat{y}_k^n) \\quad \\blacksquare\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b) {-}\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c) {-}\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d) {-}\n",
    "Early stopping occured on Epoch number 33 when not using shuffling. (Note: From this task on, every plot has early stopping enabled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e) {-}\n",
    "\n",
    "As we see from the plot below, the validation accuracy has periodic spikes. This is in fact periodic with the epoch,but as we are updating the validation accuracy every 6th batch and LCM(6, num_batches) = 6*31 = 186, the spike in the validation accuracy is only noticable every 186th global step. The reason for the spike seems to be due to a particulary hard training batch, that gives a massive loss. Since the examples are not shuffled, this will occur once per epoch, because the batch is the same. Due to this huge training loss, we will have a high gradient, which seems to overstep in this direction in the gradient decent. This causes the validation accuracy for this iteration to be a a lot worse, since the weights are then adjusted badly from the preceding training batch. However, if we shuffle the data, we will not have the same training-batches, so we will not get (close to impossible at least) this particularly hard training batch in every epoch, so we will see less spikes.\n",
    "\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b) {-}\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c) {-}\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d) {-}\n",
    "Yes, we see that the validation accuracy and the training accuracy diverges.\n",
    "The ever increasing difference between the training accuracy and the testing accuracy, after about 2500 training steps, is a clear sign of overfitting, as it means that the model is fitting the training data better than the unseen validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a) {-}\n",
    "\n",
    "From the task we are given that:\n",
    "\n",
    "\\begin{equation}\n",
    "    J(W) = C(W) + \\lambda R(W)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    C(W) = \\frac{1}{N}\\sum_{n=1}^{N}C^n(W), \\quad C^n(W) = -\\sum_{k=1}^{K}y_k^n \\ln(\\hat{y}_k^n)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    R(W) = ||W||^2 = \\sum_{i,j} w_{i,j}^2\n",
    "\\end{equation}\n",
    "\n",
    "Now to find the update term for softmax regression with L2 regularization:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial J}{\\partial W} &= \\frac{\\partial C(W)}{\\partial W} + \\lambda \\frac{\\partial R(W)}{\\partial W} \\\\\n",
    "    &= \\frac{1}{N}\\sum_{n=1}^{N} \\frac{\\partial C^n(W)}{\\partial W} + \\lambda \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial W}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial W} &=\n",
    "    \\lambda \\begin{bmatrix}\n",
    "\t\t\\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{1,1}} & \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{1,2}}   & \\cdots   & \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{1,I}}\\\\\n",
    "\t\t   \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{2,1}}       & \\ddots    & \\ddots & \\vdots\\\\\n",
    "\t\t   \\vdots  & \\ddots   & \\ddots & \\vdots\\\\\n",
    "\t\t   \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{K,1}}       & \\cdots    & \\cdots    & \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{K,I}}\n",
    "\t\\end{bmatrix} \\\\\n",
    "    \t &= \\lambda  \\begin{bmatrix}\n",
    "\t\t2 w_{1,1} & 2 w_{1,2}   & \\cdots   & 2 w_{1,I}\\\\\n",
    "\t\t2 w_{2,1}       & \\ddots    & \\ddots & \\vdots\\\\\n",
    "\t\t   \\vdots  & \\ddots   & \\ddots & \\vdots\\\\\n",
    "\t\t2 w_{K,1}       & \\cdots    & \\cdots    & 2 w_{K,I}\n",
    "\t\\end{bmatrix} \\quad = 2 \\lambda W\n",
    "\\end{align}\t\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C(W)}{\\partial W} = \\frac{1}{N} \\sum_{n=1}^{N} \n",
    "    \\begin{bmatrix}\n",
    "\t\t\\frac{\\partial C^n(W)}{\\partial w_{1,1}} & \\frac{\\partial C^n(W)}{\\partial w_{1,2}}   & \\cdots   & \\frac{\\partial C^n(W)}{\\partial w_{1,I}}\\\\\n",
    "\t\t\\frac{\\partial C^n(W)}{\\partial w_{2,1}}       & \\ddots    & \\ddots & \\vdots\\\\\n",
    "\t\t   \\vdots  & \\ddots   & \\ddots & \\vdots\\\\\n",
    "\t\t\\frac{\\partial C^n(W)}{\\partial w_{K,1}}       & \\cdots    & \\cdots    & \\frac{\\partial C^n(W)}{\\partial w_{K,I}}\n",
    "\t\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Using the results from Task 1 that $\\frac{\\partial C^n(W)}{\\partial w_{k,j}} = -x_j^n (y_k^n - \\hat{y}_k^n)$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C(W)}{\\partial W} = \\frac{1}{N} \\sum_{n=1}^{N} \n",
    "    \\begin{bmatrix}\n",
    "\t\t-x_1^n (y_1^n - \\hat{y}_1^n) & -x_2^n (y_1^n - \\hat{y}_1^n)   & \\cdots   & -x_I^n (y_1^n - \\hat{y}_1^n)\\\\\n",
    "\t\t-x_1^n (y_2^n - \\hat{y}_2^n)      & \\ddots    & \\ddots & \\vdots\\\\\n",
    "\t\t   \\vdots  & \\ddots   & \\ddots & \\vdots\\\\\n",
    "\t\t-x_1^n (y_K^n - \\hat{y}_K^n)       & \\cdots    & \\cdots    & -x_I^n (y_K^n - \\hat{y}_K^n)\n",
    "\t\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Defining the following vectors:\n",
    "\n",
    "\\begin{align}\n",
    "    \\boldsymbol{x}^n &= \\left[x_1^n,x_2^n,\\ldots,x_I^n \\right]^T \\\\\n",
    "    \\boldsymbol{y}^n &= \\left[y_1^n,y_2^n,\\ldots,y_K^n \\right]^T \\\\\n",
    "    \\boldsymbol{\\hat{y}}^n &= \\left[\\hat{y}_1^n,\\hat{y}_2^n,\\ldots,\\hat{y}_K^n \\right]^T\n",
    "\\end{align}\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C(W)}{\\partial W} = - \\frac{1}{N} \\sum_{n=1}^{N} (\\boldsymbol{y}^n - \\boldsymbol{\\hat{y}}^n) (\\boldsymbol{x}^{n})^T\n",
    "\\end{equation}\n",
    "\n",
    "Defining the following matrices:\n",
    "\\begin{align}\n",
    "    \\boldsymbol{X} &= \\left[\\boldsymbol{x}^1,\\boldsymbol{x}^2,\\ldots,\\boldsymbol{x}^N \\right] \\\\\n",
    "    \\boldsymbol{Y} &= \\left[\\boldsymbol{y}^1,\\boldsymbol{y}^2,\\ldots,\\boldsymbol{y}^N \\right] \\\\\n",
    "    \\boldsymbol{\\hat{Y}} &= \\left[\\boldsymbol{\\hat{y}}^1,\\boldsymbol{\\hat{y}}^2,\\ldots,\\boldsymbol{\\hat{y}}^N \\right]\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C(W)}{\\partial W} = - \\frac{1}{N}(\\boldsymbol{Y} - \\boldsymbol{\\hat{Y}}) \\boldsymbol{X}^T\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\implies  \\frac{\\partial J}{\\partial W} = - \\frac{1}{N}(\\boldsymbol{Y} - \\boldsymbol{\\hat{Y}}) \\boldsymbol{X}^T + 2 \\lambda W \\quad \\blacksquare\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b) {-}\n",
    "The weights with $\\lambda = 1$ is less noisy, because the L2 regularization punishes high weights. This causes the neural network to prioritize the weighting on pixels in the images that is the most relevant for classification, which for a 1 layer network naturally has the shape of the number itself, while pixels outside this area will have less weighting to lower the weight cost of L2, as they dont influence the classification accuracy that much.\n",
    "\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c) {-}\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d) {-}\n",
    "\n",
    "Considering the bias-variance trade off, regularisation constrains a model and makes it less flexible and more rigid. In other words, it will have less variance at the risk of increased bias. This means that the fitted model will be less overfit, but more likely to underfit depending on the extent of regularisation.\n",
    "\n",
    "Considering that our neural network only has 1 layer, we already have a relatively simple network, which might cause the network having too little flexibility when regularisation is added to the cost function, so the network can perform worse with it.\n",
    "\n",
    "From the figure in 4c, we see that the validation accuracy decreases for increasing values of lambda, until it becomes unstable when lambda = 1. This is a clear sign that the model is not overfitting the data from before, and that regularisation only constrains it to the point where it is no longer able to fit the data as well, as it starts to underfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e) {-}\n",
    "From the plot, we can see that the L2 norm decreases with increasing $\\lambda$, which is also what we would expect, since the L2 norm punishes exactly this.\n",
    "\n",
    "![](task4e_l2_reg_norms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}