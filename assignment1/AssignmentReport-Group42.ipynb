{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "## Logistic Regression\n",
    "\n",
    "From the assignment we are given that:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = f(x) = \\frac{1}{1 + e^{-w^T x}}, \\quad w^T x = \\sum_{i=1}^{I} w_i x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    C(w) = \\frac{1}{N}\\sum_{n=1}^{N} C^n, \\quad C^n(w) = -\\left( y^n \\ln{(\\hat{y}^n)} + (1-\\hat{y}^n)\\ln{(1-\\hat{y}^n)} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "We want to find the gradient of the cost function for training example n, $C^n$, with respect to the weight, $w_i$:\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial C^n(w)}{\\partial w_i} &= \\frac{\\partial C^n(w)}{\\partial f(x^n)} \\frac{\\partial f(x^n)}{\\partial w_i} \\\\\n",
    "&= \\underbrace{-\\left(y^n \\frac{1}{\\hat{y}^n} + \\frac{1-y^n}{1-\\hat{y}^n}(-1)\\right)}_{\\frac{\\partial C^n(w)}{\\partial f(x^n)}} \\underbrace{x_i^n f(x^n) (1-f(x^n))}_{\\frac{\\partial f(x^n)}{\\partial w_i}}, \\quad f(x^n) = \\hat{y}^n \\\\\n",
    "&= -\\left( y^n(1-\\hat{y}^n) - (1-y^n)\\hat{y}^n \\right) x_i^n \\\\\n",
    "&= -(y^n-\\hat{y}^n) x_i^n\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "source": [
    "## task 1b)\n",
    "\n",
    "## Softmax Regression\n",
    "\n",
    "From the assignment we have that:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y}_k = \\frac{e^{z_k}}{\\sum_{k'=1}^{K} e^{z_{k'}}}, \\quad z_k = w_k^T x = \\sum_{i=1}^{I} w_{k,i} x_i\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    C(w) = \\frac{1}{N}\\sum_{n=1}^{N} C^n, \\quad C^n(w) = - \\sum_{k=1}^{K} y_k^n \\ln{(\\hat{y}_k^n)}\n",
    "\\end{equation}\n",
    "\n",
    "We want to find the gradient of the cost function for training example n, $C^n$, with respect to the weight, $w_{kj}$:\n",
    "\n",
    "We start by rewriting the $C^n$ to another equivalent form\n",
    "\\begin{align*}\n",
    "    C^n(w) &= - \\sum_{k=1}^{K} y_k^n \\ln{\\left(\\frac{e^{z_k}}{\\sum_{k'=1}^{K} e^{z_{k'}}}\\right)} \\\\\n",
    "    &= -\\sum_{k=1}^{K} y_k^n \\ln{( e^{z_k})} + \\sum_{k=1}^{K} y_k^n \\ln{\\left( \\sum_{k'=1}^{K} e^{z_{k'}} \\right)} \\\\\n",
    "    &= -\\sum_{k=1}^{K} y_k^n \\underbrace{\\ln{( e^{z_k})}}_{z_k} + \\ln{\\left( \\sum_{k'=1}^{K} e^{z_{k'}} \\right)} \\underbrace{\\sum_{k=1}^{K} y_k^n}_{1}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\implies C^n(w) = -\\sum_{k=1}^{K} y_k^n z_k + \\ln{\\left( \\sum_{k'=1}^{K} e^{z_{k'}} \\right)}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial C^n(w)}{\\partial w_{kj}} &= \\frac{\\partial C^n(w)}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}} \\\\\n",
    "    &= \\left(-y_k^n + \\frac{e^{z_k}}{\\sum_{k'=1}^{K} e^{z_{k'}}}  \\right) x_j^n \\\\\n",
    "    &= (-y_k^n + \\hat{y}_k^n) x_j^n\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C^n(w)}{\\partial w_{kj}} = -x_j^n (y_k^n - \\hat{y}_k^n)\n",
    "\\end{equation}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Task 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Early stopping occured on Epoch number 33."
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "As we see from the plot, the validation loss has periodic spike. This is in fact periodic with the epoch, but to the validation loss having the conditional check for then to check validation global_step % num_steps_per_val, where num_steps_per_val = 6 and each batch containing 31 images in the starter code, we will in the plot see this spike for every 31*6 = 186 global_step. The reason for the spike seems to be due to a particulary hard training batch, that gives a massive loss, which can be seen in the training loss graph(the other similar peaks correspond to the other validation loss peaks we lose to the modulo check explained earlier). Since the examples are not shuffled, this will occur once per epoch, because the batch is the same. Due to this huge training loss, we will have a high gradient, which seems to overstep in this direction in the gradient decent. This causes the iteration in the validation loss in the same iteration to have a much larger loss, since the weights are then adjusted badly from the preceding training example. However, if we shuffle the data, we will not have the same batch for every epoch, so we will not get (close to impossible atleast) this particularly hard training batch in every epoch, so we will see less spikes.\n",
    "![](task2e_train_accuracy_shuffle_difference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "![](task3b_softmax_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "![](task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "Yes, we see that the validation accuracy and the training accuracy diverges.\n",
    "The ever increasing difference between the training accuracy and the testing accuracy, after about 2500 training steps, is a clear sign of overfitting, as it means that the model is fitting the training data better than the unseen validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "\\begin{equation}\n",
    "    J(W) = C(W) + \\lambda R(W)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    C(W) = \\frac{1}{N}\\sum_{n=1}^{N}C^n(W), \\quad C^n(W) = -\\sum_{k=1}^{K}y_k^n \\ln(\\hat{y}_k^n)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    R(W) = ||W||^2 = \\sum_{i,j} w_{i,j}^2\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align}\n",
    "    \\frac{\\partial J}{\\partial W} &= \\frac{\\partial C(W)}{\\partial W} + \\lambda \\frac{\\partial R(W)}{\\partial W} \\\\\n",
    "    &= \\frac{1}{N}\\sum_{n=1}^{N} \\frac{\\partial C^n(W)}{\\partial W} + \\lambda \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial W}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\lambda \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial W} &=\n",
    "    \\lambda \\begin{bmatrix}\n",
    "\t\t\\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{1,1}} & \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{1,2}}   & \\cdots   & \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{1,I}}\\\\\n",
    "\t\t   \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{2,1}}       & \\ddots    & \\ddots & \\vdots\\\\\n",
    "\t\t   \\vdots  & \\ddots   & \\ddots & \\vdots\\\\\n",
    "\t\t   \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{K,1}}       & \\cdots    & \\cdots    & \\sum_{i,j} \\frac{\\partial w_{i,j}^2}{\\partial w_{K,I}}\n",
    "\t\\end{bmatrix}\n",
    "\\end{align}\n",
    "\\begin{equation}\n",
    "    \t = \\lambda  \\begin{bmatrix}\n",
    "\t\t2 w_{1,1} & 2 w_{1,2}   & \\cdots   & 2 w_{1,I}\\\\\n",
    "\t\t2 w_{2,1}       & \\ddots    & \\ddots & \\vdots\\\\\n",
    "\t\t   \\vdots  & \\ddots   & \\ddots & \\vdots\\\\\n",
    "\t\t2 w_{K,1}       & \\cdots    & \\cdots    & 2 w_{K,I}\n",
    "\t\\end{bmatrix} \\quad = 2 \\lambda W\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C(W)}{\\partial W} = \\frac{1}{N} \\sum_{n=1}^{N} \n",
    "    \\begin{bmatrix}\n",
    "\t\t\\frac{\\partial C^n(W)}{\\partial w_{1,1}} & \\frac{\\partial C^n(W)}{\\partial w_{1,2}}   & \\cdots   & \\frac{\\partial C^n(W)}{\\partial w_{1,I}}\\\\\n",
    "\t\t\\frac{\\partial C^n(W)}{\\partial w_{2,1}}       & \\ddots    & \\ddots & \\vdots\\\\\n",
    "\t\t   \\vdots  & \\ddots   & \\ddots & \\vdots\\\\\n",
    "\t\t\\frac{\\partial C^n(W)}{\\partial w_{K,1}}       & \\cdots    & \\cdots    & \\frac{\\partial C^n(W)}{\\partial w_{K,I}}\n",
    "\t\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Using the results of Task 1 for $$\\frac{\\partial C^n(W)}{\\partial w_{k,j}}$$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C(W)}{\\partial W} = \\frac{1}{N} \\sum_{n=1}^{N} \n",
    "    \\begin{bmatrix}\n",
    "\t\t-x_1^n (y_1^n - \\hat{y}_1^n) & -x_2^n (y_1^n - \\hat{y}_1^n)   & \\cdots   & -x_I^n (y_1^n - \\hat{y}_1^n)\\\\\n",
    "\t\t-x_1^n (y_2^n - \\hat{y}_2^n)      & \\ddots    & \\ddots & \\vdots\\\\\n",
    "\t\t   \\vdots  & \\ddots   & \\ddots & \\vdots\\\\\n",
    "\t\t-x_1^n (y_K^n - \\hat{y}_K^n)       & \\cdots    & \\cdots    & -x_I^n (y_K^n - \\hat{y}_K^n)\n",
    "\t\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Defining the following vectors:\n",
    "\n",
    "\\begin{align}\n",
    "    \\boldsymbol{x}^n &= \\left[x_1^n,x_2^n,\\ldots,x_I^n \\right]^T \\\\\n",
    "    \\boldsymbol{y}^n &= \\left[y_1^n,y_2^n,\\ldots,y_K^n \\right]^T \\\\\n",
    "    \\boldsymbol{\\hat{y}}^n &= \\left[\\hat{y}_1^n,\\hat{y}_2^n,\\ldots,\\hat{y}_K^n \\right]^T\n",
    "\\end{align}\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C(W)}{\\partial W} = - \\frac{1}{N} \\sum_{n=1}^{N} (\\boldsymbol{y}^n - \\boldsymbol{\\hat{y}}^n) (\\boldsymbol{x}^{n})^T\n",
    "\\end{equation}\n",
    "\n",
    "Defining the following matrices:\n",
    "\\begin{align}\n",
    "    \\boldsymbol{X} &= \\left[\\boldsymbol{x}^1,\\boldsymbol{x}^2,\\ldots,\\boldsymbol{x}^N \\right] \\\\\n",
    "    \\boldsymbol{Y} &= \\left[\\boldsymbol{y}^1,\\boldsymbol{y}^2,\\ldots,\\boldsymbol{y}^N \\right] \\\\\n",
    "    \\boldsymbol{\\hat{Y}} &= \\left[\\boldsymbol{\\hat{y}}^1,\\boldsymbol{\\hat{y}}^2,\\ldots,\\boldsymbol{\\hat{y}}^N \\right]\n",
    "\\end{align}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial C(W)}{\\partial W} = - \\frac{1}{N}(\\boldsymbol{Y} - \\boldsymbol{\\hat{Y}}) \\boldsymbol{X}^T\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\implies  \\frac{\\partial J}{\\partial W} = - \\frac{1}{N}(\\boldsymbol{Y} - \\boldsymbol{\\hat{Y}}) \\boldsymbol{X}^T + 2 \\lambda W\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "The weights with $\\lambda = 1$ is less noisy, because the L2 regularization punishes high weights. This causes the neural network to prioritize the weighting on pixels in the images that is the most relevant for classification, which for a 1 layer network naturally has the shape of the number itself, while pixels outside this area will have less weighting to lower the weight cost of L2, as they dont influence the classification accuracy that much.\n",
    "\n",
    "![](task4b_softmax_weight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "FILL IN ANSWER\n",
    "\n",
    "![](task4c_l2_reg_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "We see that the L2 norm decreases with increasing $\\lambda$, which is also what we would expect, since the L2 norm punishes exactly this.\n",
    "![](task4e_l2_reg_norms.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}